# Agent-as-a-Judge: 자동 평가 시스템

## Overview

작업 완료 후 AI가 자동으로 품질을 평가하는 시스템.

## Evaluation Criteria

```yaml
quality_metrics:
  code_quality:
    weight: 0.30
    checks:
      - SOLID principles adherence
      - DRY, KISS, YAGNI compliance
      - Code readability
      - Maintainability score

  efficiency:
    weight: 0.25
    checks:
      - Optimal approach used
      - Time complexity
      - Space complexity
      - Token efficiency

  completeness:
    weight: 0.25
    checks:
      - All requirements met
      - Edge cases handled
      - Error handling present
      - Documentation complete

  evidence:
    weight: 0.20
    checks:
      - Metrics provided
      - Test results included
      - Verification performed
      - File references (file:line)
```

## Scoring System

```yaml
scores:
  excellent: 0.90 - 1.00  # Exceptional quality
  good: 0.75 - 0.89       # High quality, minor improvements
  acceptable: 0.60 - 0.74 # Meets standards, some issues
  needs_work: 0.40 - 0.59 # Significant improvements needed
  poor: 0.00 - 0.39       # Major revisions required
```

## Evaluation Template

```markdown
# Auto-Evaluation Report: [Task Name]

**Date**: YYYY-MM-DD HH:MM
**Evaluator**: Agent-as-a-Judge v1.0
**Task ID**: [task-id]

---

## Overall Score: X.XX / 1.00 ([Rating])

### 1. Code Quality (30%): X.XX / 0.30
**SOLID Principles**: [Pass/Fail/Partial]
- Single Responsibility: [✅/⚠️/❌] [comment]
- Open/Closed: [✅/⚠️/❌] [comment]
- Liskov Substitution: [✅/⚠️/❌] [comment]
- Interface Segregation: [✅/⚠️/❌] [comment]
- Dependency Inversion: [✅/⚠️/❌] [comment]

**Code Patterns**: [✅/⚠️/❌]
- DRY: [comment]
- KISS: [comment]
- YAGNI: [comment]

**Readability**: [✅/⚠️/❌]
- [detailed assessment]

### 2. Efficiency (25%): X.XX / 0.25
**Approach**: [Optimal/Acceptable/Suboptimal]
- [reasoning]

**Complexity**: [✅/⚠️/❌]
- Time: O(?)
- Space: O(?)

**Token Usage**: [Efficient/Standard/Wasteful]
- [metrics]

### 3. Completeness (25%): X.XX / 0.25
**Requirements**: [✅/⚠️/❌]
- [requirement 1]: [status]
- [requirement 2]: [status]

**Edge Cases**: [✅/⚠️/❌]
- [cases covered]

**Error Handling**: [✅/⚠️/❌]
- [assessment]

### 4. Evidence (20%): X.XX / 0.20
**Metrics Provided**: [✅/⚠️/❌]
- [list metrics]

**Test Results**: [✅/⚠️/❌]
- [test coverage, pass rate]

**Verification**: [✅/⚠️/❌]
- [verification steps]

**References**: [✅/⚠️/❌]
- [file:line citations]

---

## Strengths
1. [strength 1]
2. [strength 2]

## Areas for Improvement
1. [improvement 1]
2. [improvement 2]

## Recommendations
1. [actionable recommendation 1]
2. [actionable recommendation 2]

## Follow-up Actions
- [ ] [action 1]
- [ ] [action 2]

---
*Generated by Agent-as-a-Judge v1.0*
*Evaluation Framework: Context Engineering Quality Standards*
```

## Usage

### Trigger Points
- Post-task completion hook
- Manual evaluation request
- Pre-commit quality gate
- Wave completion validation

### Integration
```yaml
hooks:
  Stop:
    - agent-judge-eval.py

commands:
  - /moon-review (uses Agent-as-a-Judge)
  - /verify-app (includes evaluation)
```

## Implementation Status

- [x] Directory structure
- [x] Evaluation criteria defined
- [x] Scoring system established
- [x] Template created
- [ ] Evaluation prompt engineering
- [ ] Hook integration
- [ ] Metrics tracking

## Expected Impact

**Quality Improvement**: +30%
**Consistency**: 95%+ evaluation reliability
**Learning**: Pattern accumulation for future improvements

## Next Steps

1. Create evaluation prompts
2. Implement post-task hook
3. Integrate with evolution feedback
4. Test with completed tasks
5. Refine scoring weights
